# ğŸ“Š Data-Pipeline: Korrelation zwischen Bitcoin und BÃ¶rsen

## ğŸ“„ Projektbeschreibung

Dieses Projekt analysiert die Korrelation zwischen der KryptowÃ¤hrung Bitcoin und traditionellen BÃ¶rsenindizes wie dem Dow Jones Industrial Average (DJIA). Die Daten stammen von der Plattform [Kaggle](https://www.kaggle.com/) und enthalten Ã¼ber 1 Million Datenpunkte, die tÃ¤glich bzw. minÃ¼tlich aktualisiert werden.

Ziel ist es, mithilfe moderner Big-Data-Technologien aussagekrÃ¤ftige Analysen Ã¼ber mÃ¶gliche ZusammenhÃ¤nge zwischen traditionellen und digitalen FinanzmÃ¤rkten durchzufÃ¼hren.

---

## ğŸ“ Projektstruktur

```text
data-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ bitcoin/
â”‚   â”‚   â”‚   â””â”€â”€ btcusd_1-min_data.csv
â”‚   â”‚   â””â”€â”€ djia/
â”‚   â”‚       â”œâ”€â”€ dow_jones_data.csv
â”‚   â”‚       â””â”€â”€ djia_clean.csv
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ combined_daily_and_summary.csv
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ correlation_btc_close.png
â”‚   â””â”€â”€ correlation_djia_close.png
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ kaggle/
â”‚       â”œâ”€â”€ kaggle.json (nicht im Git enthalten)
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ process.py
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ mysql/
â”‚   â””â”€â”€ init.sql
â”œâ”€â”€ elk/
â”‚   â””â”€â”€ logstash.conf
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ run_pipeline.bat
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env (nicht im Git enthalten)
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

âš™ï¸ Technologien

    Docker & Docker-Compose: Containerisierung und Orchestrierung der Microservices

    Apache Spark: Verarbeitung, Aggregation und Analyse groÃŸer Datenmengen

    MySQL: Persistente Speicherung der bereinigten und aggregierten Daten

    ELK Stack (Elasticsearch, Logstash, Kibana): Visualisierung und Ãœberwachung von Logs und Status

    Kaggle API: Download der Rohdaten

    GitHub: Versionierung und Codeverwaltung

    Python dotenv: Verwaltung von Umgebungsvariablen (.env-Dateien)

ğŸ”„ Datenpipeline-Ablauf

    Datenbeschaffung
    Die historischen Daten zu Bitcoin und DJIA werden automatisch per Kaggle-API heruntergeladen.

    Ingestion Service
    Die CSV-Dateien werden in ein temporÃ¤res Verzeichnis geschrieben.

    Verarbeitung mit Spark
    Start Ã¼ber Docker Compose
    Bereinigung, Aggregation und Korrelation der Daten
    Speicherung in MySQL

    Ãœberwachung
    ELK-Stack visualisiert Logs und den Zustand des Systems in Echtzeit.

ğŸ§± Architekturprinzipien

    Microservice-Architektur zur Trennung von ZustÃ¤ndigkeiten

    Skalierbarkeit durch Spark & Docker

    Sicherheit durch eingeschrÃ¤nkte Netzwerkbereiche und Zugang zu Services

    Einhaltung von Datenschutz und Data-Governance-Prinzipien

ğŸ”— Datenquellen

    Bitcoin Historical Data â€“ minÃ¼tlich

    DJIA Historical Data â€“ tÃ¤glich

## Visualisierungen

### Bitcoin Close Price Correlation

![Correlation Bitcoin](images/correlation_btc_close.png)

### DJIA Close Price Correlation

![Correlation DJIA](images/correlation_djia_close.png)

Correlation DJIA
ğŸ–¥ï¸ Manuelle AusfÃ¼hrung beim ersten Start (Windows)

    Stelle sicher, dass Docker Desktop gestartet ist.

    Navigiere im Explorer zum Projektverzeichnis.

    FÃ¼hre die Datei run_pipeline.bat per Doppelklick aus.

    Alternativ im Terminal:

cd "Pfad\zum\Projektordner"
run_pipeline.bat

Installiere benÃ¶tigte Python-Bibliotheken:

pip install -r requirements.txt

Erstelle eine .env Datei mit folgendem Inhalt:

DB_HOST=localhost
DB_USER=root
DB_PASSWORD=geheim
DB_NAME=finance

Lade deine kaggle.json von deinem Kaggle-Konto herunter und speichere sie unter:

    ingestion/kaggle/kaggle.json

    Die Ausgabedateien findest du nach der Verarbeitung unter data/processed/.

â° Automatisierte AusfÃ¼hrung mit dem Windows Task Scheduler

Die Pipeline kann einmal pro Quartal automatisch ausgefÃ¼hrt werden.
Schritt-fÃ¼r-Schritt Anleitung:

    Taskplaner Ã¶ffnen
    Suche im StartmenÃ¼ nach "Aufgabenplanung" oder "Task Scheduler".

    Neue Aufgabe erstellen
    Klicke auf â€Aufgabe erstellenâ€¦â€œ (nicht â€Einfache Aufgabeâ€œ).

    Registerkarte Allgemein
    Name: Data Pipeline Quarterly
    Haken bei â€Mit hÃ¶chsten Privilegien ausfÃ¼hrenâ€œ setzen

    Trigger
    Zeitplan: Monatlich
    WÃ¤hle z.â€¯B. Januar, April, Juli, Oktober
    Tag: z.â€¯B. 1., Uhrzeit: 10:00

    Aktionen
    Neue Aktion â†’ â€Programm startenâ€œ
    Pfad zur .bat-Datei:
    C:\Pfad\zum\Projekt\run_pipeline.bat

    Bedingungen & Einstellungen (optional)
    Nur bei Netzstrom ausfÃ¼hren
    Bei Bedarf auch bei InaktivitÃ¤t

    Speichern und testen
    Rechtsklick auf den Task â†’ â€AusfÃ¼hrenâ€œ testen